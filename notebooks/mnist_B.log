Using TensorFlow backend.
2017-12-28 15:14:42.692011: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:42.692057: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:42.692064: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:42.692069: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:42.692074: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:42.866747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 9.26GiB
2017-12-28 15:14:43.014683: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x99aff00 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-12-28 15:14:43.015526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:04:00.0
Total memory: 11.92GiB
Free memory: 11.27GiB
2017-12-28 15:14:43.015706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2017-12-28 15:14:43.015725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y 
2017-12-28 15:14:43.015730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y 
2017-12-28 15:14:43.015741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
2017-12-28 15:14:43.015747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)
../cleverhans/cleverhans/utils_tf.py:112: UserWarning: verbose argument is deprecated and will be removed on 2018-02-11. Instead, use utils.set_log_level(). For backward compatibility, log_level was set to logging.WARNING (30).
  warnings.warn("verbose argument is deprecated and will be removed"
(50000, 28, 28, 1)
[#] Max: 1.0, Min: 0.0
[*] Label : 3.0
[ 3.  4.  8. ...,  0.  1.  2.]
[ 3.  4.  8. ...,  0.  1.  2.]
0.9785
<tensorflow.python.client.session.Session object at 0x92babd0>
 0/5: loss: 2.38276433945 0/5: loss: 2.25579786301 0/5: loss: 2.12618422508 0/5: loss: 1.65924978256 0/5: loss: 1.42509293556 0/5: loss: 1.56623828411 0/5: loss: 1.27952420712 0/5: loss: 0.940554499626 0/5: loss: 1.07858490944 0/5: loss: 0.897914111614 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/10: loss: 1.95976066589 0/10: loss: 1.20456135273 0/10: loss: 1.35888230801 0/10: loss: 1.27415394783 0/10: loss: 1.30973982811 0/10: loss: 1.24478292465 0/10: loss: 1.26555967331 0/10: loss: 1.12105762959 0/10: loss: 1.24219298363 0/10: loss: 1.13686561584 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/19: loss: 0.929884433746 10/19: loss: 0.757167100906 0/19: loss: 0.948887944221 10/19: loss: 0.815470039845 0/19: loss: 0.804857254028 10/19: loss: 0.998190939426 0/19: loss: 1.03668630123 10/19: loss: 0.733350038528 0/19: loss: 0.970615327358 10/19: loss: 0.43100258708 0/19: loss: 0.684728384018 10/19: loss: 0.748602986336 0/19: loss: 0.716204106808 10/19: loss: 0.958422064781 0/19: loss: 0.457863807678 10/19: loss: 0.597378253937 0/19: loss: 0.503690719604 10/19: loss: 1.10053396225 0/19: loss: 0.753342270851 10/19: loss: 0.491504490376 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/38: loss: 0.379618108273 10/38: loss: 0.511735916138 20/38: loss: 0.277362555265 30/38: loss: 0.384975254536 0/38: loss: 0.247033163905 10/38: loss: 0.436696261168 20/38: loss: 0.655903935432 30/38: loss: 0.50622522831 0/38: loss: 0.338514447212 10/38: loss: 0.59032201767 20/38: loss: 0.406211674213 30/38: loss: 0.391889154911 0/38: loss: 1.01496767998 10/38: loss: 0.240081340075 20/38: loss: 0.577034771442 30/38: loss: 0.48605453968 0/38: loss: 0.436203807592 10/38: loss: 0.276268839836 20/38: loss: 0.541026353836 30/38: loss: 0.372564584017 0/38: loss: 0.345668494701 10/38: loss: 0.662508130074 20/38: loss: 0.0716209933162 30/38: loss: 0.50180876255 0/38: loss: 0.682576417923 10/38: loss: 0.192930310965 20/38: loss: 0.295915722847 30/38: loss: 0.433445215225 0/38: loss: 0.456794828176 10/38: loss: 0.125467807055 20/38: loss: 0.565201163292 30/38: loss: 0.469203352928 0/38: loss: 0.539270877838 10/38: loss: 0.309324026108 20/38: loss: 0.315091013908 30/38: loss: 0.243300139904 0/38: loss: 0.433922111988 10/38: loss: 0.653905510902 20/38: loss: 0.323378205299 30/38: loss: 0.187653779984 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/75: loss: 0.276454925537 10/75: loss: 0.226626381278 20/75: loss: 0.141773149371 30/75: loss: 0.0648390352726 40/75: loss: 0.0986565575004 50/75: loss: 0.132758021355 60/75: loss: 0.396570086479 70/75: loss: 0.0883904248476 0/75: loss: 0.174229979515 10/75: loss: 0.308255881071 20/75: loss: 0.241064161062 30/75: loss: 0.136785671115 40/75: loss: 0.188370212913 50/75: loss: 0.338295519352 60/75: loss: 0.401795595884 70/75: loss: 0.814163565636 0/75: loss: 0.29589599371 10/75: loss: 0.167178094387 20/75: loss: 0.470971167088 30/75: loss: 0.153340235353 40/75: loss: 0.0721354112029 50/75: loss: 0.405783116817 60/75: loss: 0.349084705114 70/75: loss: 0.0692264437675 0/75: loss: 0.216562777758 10/75: loss: 0.468347907066 20/75: loss: 0.0515623055398 30/75: loss: 0.178367614746 40/75: loss: 0.176967859268 50/75: loss: 0.213781520724 60/75: loss: 0.068752206862 70/75: loss: 0.258381009102 0/75: loss: 0.0468308143318 10/75: loss: 0.317738622427 20/75: loss: 0.417800962925 30/75: loss: 0.334468394518 40/75: loss: 0.0988969504833 50/75: loss: 0.103211656213 60/75: loss: 0.057930290699 70/75: loss: 0.198094516993 0/75: loss: 0.179109275341 10/75: loss: 0.130072087049 20/75: loss: 0.151410147548 30/75: loss: 0.329647213221 40/75: loss: 0.18084782362 50/75: loss: 0.297103196383 60/75: loss: 0.0638112351298 70/75: loss: 0.539445579052 0/75: loss: 0.193345814943 10/75: loss: 0.219164997339 20/75: loss: 0.198595166206 30/75: loss: 0.229182720184 40/75: loss: 0.176183715463 50/75: loss: 0.0802182853222 60/75: loss: 0.226192846894 70/75: loss: 0.404059022665 0/75: loss: 0.615361094475 10/75: loss: 0.165481984615 20/75: loss: 0.234517350793 30/75: loss: 0.0428082421422 40/75: loss: 0.302588075399 50/75: loss: 0.184969469905 60/75: loss: 0.820054113865 70/75: loss: 0.270406544209 0/75: loss: 0.249305009842 10/75: loss: 0.332575589418 20/75: loss: 0.125472247601 30/75: loss: 0.0475175566971 40/75: loss: 0.0206045731902 50/75: loss: 0.0449240170419 60/75: loss: 0.344483852386 70/75: loss: 0.296770781279 0/75: loss: 0.150235623121 10/75: loss: 0.336270213127 20/75: loss: 0.256962537766 30/75: loss: 0.373777985573 40/75: loss: 0.425276041031 50/75: loss: 0.105133108795 60/75: loss: 0.057375408709 70/75: loss: 0.454341113567 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/150: loss: 0.0203827042133 10/150: loss: 0.00237259664573 20/150: loss: 0.210399925709 30/150: loss: 0.0149232121184 40/150: loss: 0.0343893654644 50/150: loss: 0.255310624838 60/150: loss: 0.0297490991652 70/150: loss: 0.0175429116935 80/150: loss: 0.184448733926 90/150: loss: 0.144152194262 100/150: loss: 0.102995067835 110/150: loss: 0.0255439635366 120/150: loss: 0.0219345204532 130/150: loss: 0.0218127034605 140/150: loss: 0.0608536079526 0/150: loss: 0.0478758104146 10/150: loss: 0.436411440372 20/150: loss: 0.341164499521 30/150: loss: 0.0775338709354 40/150: loss: 0.432830780745 50/150: loss: 0.0523090809584 60/150: loss: 0.00590782333165 70/150: loss: 0.292134404182 80/150: loss: 0.0347417965531 90/150: loss: 0.0418973937631 100/150: loss: 0.108419723809 110/150: loss: 0.0633676052094 120/150: loss: 0.0943179875612 130/150: loss: 0.272575885057 140/150: loss: 0.00493651349097 0/150: loss: 0.078868418932 10/150: loss: 0.0908869802952 20/150: loss: 0.052673228085 30/150: loss: 0.240863516927 40/150: loss: 0.0638670176268 50/150: loss: 0.161538884044 60/150: loss: 0.0267353206873 70/150: loss: 0.0904161334038 80/150: loss: 0.0191620886326 90/150: loss: 0.0282965805382 100/150: loss: 0.217108100653 110/150: loss: 0.0311390757561 120/150: loss: 0.25181761384 130/150: loss: 0.148360788822 140/150: loss: 0.0441067814827 0/150: loss: 0.00597830768675 10/150: loss: 0.0132737988606 20/150: loss: 0.0673927366734 30/150: loss: 0.0608792938292 40/150: loss: 0.333584725857 50/150: loss: 0.181888252497 60/150: loss: 0.024796128273 70/150: loss: 0.0177829209715 80/150: loss: 0.0147937154397 90/150: loss: 0.0385269932449 100/150: loss: 0.246583819389 110/150: loss: 0.284809291363 120/150: loss: 0.00664860755205 130/150: loss: 0.256197243929 140/150: loss: 0.260353177786 0/150: loss: 0.243427947164 10/150: loss: 0.0393360219896 20/150: loss: 0.0311031192541 30/150: loss: 0.0175746697932 40/150: loss: 0.00656732451171 50/150: loss: 0.0362936519086 60/150: loss: 0.0664772391319 70/150: loss: 0.0656887739897 80/150: loss: 0.170599639416 90/150: loss: 0.365484476089 100/150: loss: 0.00693083461374 110/150: loss: 0.0861654505134 120/150: loss: 0.0653492435813 130/150: loss: 0.0816157460213 140/150: loss: 0.108662292361 0/150: loss: 0.036525644362 10/150: loss: 0.0206061918288 20/150: loss: 0.0589340031147 30/150: loss: 0.0916369557381 40/150: loss: 0.25446665287 50/150: loss: 0.122266046703 60/150: loss: 0.0960570722818 70/150: loss: 0.0242933705449 80/150: loss: 0.0169665999711 90/150: loss: 0.0232202000916 100/150: loss: 0.0208489056677 110/150: loss: 0.190908193588 120/150: loss: 0.111889153719 130/150: loss: 0.194393694401 140/150: loss: 0.028128053993 0/150: loss: 0.116409286857 10/150: loss: 0.166340470314 20/150: loss: 0.116852901876 30/150: loss: 0.0506588220596 40/150: loss: 0.0440593287349 50/150: loss: 0.207753509283 60/150: loss: 0.223636358976 70/150: loss: 0.0380978733301 80/150: loss: 0.339236050844 90/150: loss: 0.0279525406659 100/150: loss: 0.0893142521381 110/150: loss: 0.0145126376301 120/150: loss: 0.322740256786 130/150: loss: 0.306733161211 140/150: loss: 0.104598902166 0/150: loss: 0.00804756768048 10/150: loss: 0.0194254424423 20/150: loss: 0.0532372482121 30/150: loss: 0.0286303237081 40/150: loss: 0.0134137608111 50/150: loss: 0.0422173961997 60/150: loss: 0.0526113323867 70/150: loss: 0.169419080019 80/150: loss: 0.0819041281939 90/150: loss: 0.0355263762176 100/150: loss: 0.0756860524416 110/150: loss: 0.0735406726599 120/150: loss: 0.0225010551512 130/150: loss: 0.0206043161452 140/150: loss: 0.069047883153 0/150: loss: 0.10095538944 10/150: loss: 0.0848746150732 20/150: loss: 0.456869274378 30/150: loss: 0.668078660965 40/150: loss: 0.0272457152605 50/150: loss: 0.125161647797 60/150: loss: 0.23953551054 70/150: loss: 0.00644523929805 80/150: loss: 0.0123128928244 90/150: loss: 0.0407483018935 100/150: loss: 0.404359996319 110/150: loss: 0.0702209323645 120/150: loss: 0.0884027928114 130/150: loss: 0.110269591212 140/150: loss: 0.0627892315388 0/150: loss: 0.140183642507 10/150: loss: 0.0977295041084 20/150: loss: 0.10254060477 30/150: loss: 0.0183893479407 40/150: loss: 0.0566812977195 50/150: loss: 0.120205610991 60/150: loss: 0.0734011605382 70/150: loss: 0.0677272081375 80/150: loss: 0.0560025237501 90/150: loss: 0.0404296554625 100/150: loss: 0.241457074881 110/150: loss: 0.0664715990424 120/150: loss: 0.0632300376892 130/150: loss: 0.254745185375../cleverhans/cleverhans/attacks.py:39: UserWarning: CleverHans support for supplying a callable instead of an instance of the cleverhans.model.Model class is deprecated and will be dropped on 2018-01-11.
  warnings.warn("CleverHans support for supplying a callable"
 140/150: loss: 0.23838698864 [*] Done with training 
 [-] Eval batch 0/58: acc: 0.939393939394 [-] Eval batch 1/58: acc: 0.938461538462 [-] Eval batch 2/58: acc: 0.958762886598 [-] Eval batch 3/58: acc: 0.961240310078 [-] Eval batch 4/58: acc: 0.95652173913 [-] Eval batch 5/58: acc: 0.958549222798 [-] Eval batch 6/58: acc: 0.946666666667 [-] Eval batch 7/58: acc: 0.937743190661 [-] Eval batch 8/58: acc: 0.941176470588 [-] Eval batch 9/58: acc: 0.947040498442 [-] Eval batch 10/58: acc: 0.937677053824 [-] Eval batch 11/58: acc: 0.935064935065 [-] Eval batch 12/58: acc: 0.928057553957 [-] Eval batch 13/58: acc: 0.928730512249 [-] Eval batch 14/58: acc: 0.931392931393 [-] Eval batch 15/58: acc: 0.929824561404 [-] Eval batch 16/58: acc: 0.932110091743 [-] Eval batch 17/58: acc: 0.934142114385 [-] Eval batch 18/58: acc: 0.932676518883 [-] Eval batch 19/58: acc: 0.936037441498 [-] Eval batch 20/58: acc: 0.936106983655 [-] Eval batch 21/58: acc: 0.93475177305 [-] Eval batch 22/58: acc: 0.932157394844 [-] Eval batch 23/58: acc: 0.931079323797 [-] Eval batch 24/58: acc: 0.931335830212 [-] Eval batch 25/58: acc: 0.932773109244 [-] Eval batch 26/58: acc: 0.934104046243 [-] Eval batch 27/58: acc: 0.935340022297 [-] Eval batch 28/58: acc: 0.933261571582 [-] Eval batch 29/58: acc: 0.934443288241 [-] Eval batch 30/58: acc: 0.932527693857 [-] Eval batch 31/58: acc: 0.932682926829 [-] Eval batch 32/58: acc: 0.932828760643 [-] Eval batch 33/58: acc: 0.934802571166 [-] Eval batch 34/58: acc: 0.933987511151 [-] Eval batch 35/58: acc: 0.935819601041 [-] Eval batch 36/58: acc: 0.935864978903 [-] Eval batch 37/58: acc: 0.935086277732 [-] Eval batch 38/58: acc: 0.93674939952 [-] Eval batch 39/58: acc: 0.937548790008 [-] Eval batch 40/58: acc: 0.937547600914 [-] Eval batch 41/58: acc: 0.939033457249 [-] Eval batch 42/58: acc: 0.939724037763 [-] Eval batch 43/58: acc: 0.94109297374 [-] Eval batch 44/58: acc: 0.941013185288 [-] Eval batch 45/58: acc: 0.940936863544 [-] Eval batch 46/58: acc: 0.941528239203 [-] Eval batch 47/58: acc: 0.941444372154 [-] Eval batch 48/58: acc: 0.942001274697 [-] Eval batch 49/58: acc: 0.942535915053 [-] Eval batch 50/58: acc: 0.942437232088 [-] Eval batch 51/58: acc: 0.941141141141 [-] Eval batch 52/58: acc: 0.94166175604 [-] Eval batch 53/58: acc: 0.941584731058 [-] Eval batch 54/58: acc: 0.942078364566 [-] Eval batch 55/58: acc: 0.940881204685 [-] Eval batch 56/58: acc: 0.940273972603 [-] Eval batch 57/58: acc: 0.938072159397 [*] Done with testing 
Test accuracy of oracle on adversarial examples generated using the substitute: 0.941621621622

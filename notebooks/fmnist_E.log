Using TensorFlow backend.
2017-12-28 15:14:33.050923: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.051217: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.051225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.051230: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.051234: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.241382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 9.72GiB
2017-12-28 15:14:33.412582: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0xa7a4850 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-12-28 15:14:33.413429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:04:00.0
Total memory: 11.92GiB
Free memory: 11.49GiB
2017-12-28 15:14:33.413616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2017-12-28 15:14:33.413626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y 
2017-12-28 15:14:33.413630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y 
2017-12-28 15:14:33.413656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
2017-12-28 15:14:33.413668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)
../cleverhans/cleverhans/utils_tf.py:112: UserWarning: verbose argument is deprecated and will be removed on 2018-02-11. Instead, use utils.set_log_level(). For backward compatibility, log_level was set to logging.WARNING (30).
  warnings.warn("verbose argument is deprecated and will be removed"
(50000, 28, 28, 1)
[#] Max: 1.0, Min: 0.0
[*] Label : 9.0
[ 9.  8.  3. ...,  7.  0.  9.]
[ 9.  8.  3. ...,  7.  0.  9.]
0.909
<tensorflow.python.client.session.Session object at 0xa0ecb50>
 0/5: loss: 2.25192832947 0/5: loss: 1.67011547089 0/5: loss: 1.311845541 0/5: loss: 1.09693741798 0/5: loss: 0.935686528683 0/5: loss: 0.64326530695 0/5: loss: 0.676691651344 0/5: loss: 0.630650103092 0/5: loss: 0.460070550442 0/5: loss: 0.495142102242 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/10: loss: 2.24979710579 0/10: loss: 1.70843911171 0/10: loss: 1.341766119 0/10: loss: 1.22639155388 0/10: loss: 1.270652771 0/10: loss: 1.01771664619 0/10: loss: 0.94229978323 0/10: loss: 1.05270981789 0/10: loss: 0.961661934853 0/10: loss: 1.13833832741 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/19: loss: 0.948532342911 10/19: loss: 0.853353857994 0/19: loss: 0.923082709312 10/19: loss: 0.633901953697 0/19: loss: 0.484234929085 10/19: loss: 0.86171746254 0/19: loss: 0.701133549213 10/19: loss: 0.853024959564 0/19: loss: 0.56745660305 10/19: loss: 0.6780128479 0/19: loss: 0.379535108805 10/19: loss: 0.703192770481 0/19: loss: 0.62422144413 10/19: loss: 0.630587100983 0/19: loss: 0.390839487314 10/19: loss: 0.705025732517 0/19: loss: 0.362644135952 10/19: loss: 0.953220367432 0/19: loss: 0.498373866081 10/19: loss: 0.272423058748 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/38: loss: 0.393861234188 10/38: loss: 0.393102735281 20/38: loss: 0.366950929165 30/38: loss: 0.445161104202 0/38: loss: 0.372914850712 10/38: loss: 0.300554335117 20/38: loss: 0.672873020172 30/38: loss: 0.510980188847 0/38: loss: 0.299160599709 10/38: loss: 0.445111215115 20/38: loss: 0.154111132026 30/38: loss: 0.52546620369 0/38: loss: 0.589558124542 10/38: loss: 0.335205674171 20/38: loss: 0.430419802666 30/38: loss: 0.472862094641 0/38: loss: 0.273969888687 10/38: loss: 0.341189742088 20/38: loss: 0.578857719898 30/38: loss: 0.606289148331 0/38: loss: 0.246258035302 10/38: loss: 0.358091890812 20/38: loss: 0.226964697242 30/38: loss: 0.703002035618 0/38: loss: 0.481246113777 10/38: loss: 0.196412563324 20/38: loss: 0.256707191467 30/38: loss: 0.54170268774 0/38: loss: 0.241425901651 10/38: loss: 0.316512703896 20/38: loss: 0.438164353371 30/38: loss: 0.518310368061 0/38: loss: 0.452438592911 10/38: loss: 0.300948917866 20/38: loss: 0.261281132698 30/38: loss: 0.116315342486 0/38: loss: 0.372330486774 10/38: loss: 0.487380743027 20/38: loss: 0.477768957615 30/38: loss: 0.167138397694 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/75: loss: 0.951053023338 10/75: loss: 0.389181256294 20/75: loss: 0.139448657632 30/75: loss: 0.0581238716841 40/75: loss: 0.249572604895 50/75: loss: 0.458939284086 60/75: loss: 0.365967333317 70/75: loss: 0.161628752947 0/75: loss: 0.111307404935 10/75: loss: 0.330784142017 20/75: loss: 0.614945113659 30/75: loss: 0.120179794729 40/75: loss: 0.417978048325 50/75: loss: 0.141065716743 60/75: loss: 0.416548073292 70/75: loss: 1.10676145554 0/75: loss: 0.143299609423 10/75: loss: 0.236538425088 20/75: loss: 0.473175823689 30/75: loss: 0.204430803657 40/75: loss: 0.0565920695662 50/75: loss: 0.300471961498 60/75: loss: 0.0402445606887 70/75: loss: 0.0434502661228 0/75: loss: 0.374181061983 10/75: loss: 0.180462181568 20/75: loss: 0.260558724403 30/75: loss: 0.208659231663 40/75: loss: 0.249573796988 50/75: loss: 0.215746343136 60/75: loss: 0.0561430230737 70/75: loss: 0.345387816429 0/75: loss: 0.0471374094486 10/75: loss: 0.362152934074 20/75: loss: 0.556033849716 30/75: loss: 0.115719169378 40/75: loss: 0.0682861879468 50/75: loss: 0.253147661686 60/75: loss: 0.223636299372 70/75: loss: 0.112648159266 0/75: loss: 0.269858121872 10/75: loss: 0.0435197427869 20/75: loss: 0.103568837047 30/75: loss: 0.325528860092 40/75: loss: 0.20391613245 50/75: loss: 0.333423674107 60/75: loss: 0.163012951612 70/75: loss: 0.429135680199 0/75: loss: 0.0907330214977 10/75: loss: 0.126489847898 20/75: loss: 0.287511348724 30/75: loss: 0.361868947744 40/75: loss: 0.186287164688 50/75: loss: 0.403457462788 60/75: loss: 0.46554929018 70/75: loss: 0.161473155022 0/75: loss: 0.271023929119 10/75: loss: 0.293415516615 20/75: loss: 0.228233486414 30/75: loss: 0.0739805698395 40/75: loss: 0.189063817263 50/75: loss: 0.275328159332 60/75: loss: 0.331472426653 70/75: loss: 0.41361322999 0/75: loss: 0.137067168951 10/75: loss: 0.318749368191 20/75: loss: 0.0669203475118 30/75: loss: 0.0311999861151 40/75: loss: 0.174178868532 50/75: loss: 0.0535602867603 60/75: loss: 0.241098254919 70/75: loss: 0.116434328258 0/75: loss: 0.0630604997277 10/75: loss: 0.079095095396 20/75: loss: 0.172580808401 30/75: loss: 0.203878134489 40/75: loss: 0.233882963657 50/75: loss: 0.278558045626 60/75: loss: 0.058211453259 70/75: loss: 0.417870521545 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/150: loss: 0.197115153074 10/150: loss: 0.0169805511832 20/150: loss: 0.250133603811 30/150: loss: 0.333916634321 40/150: loss: 0.0813594609499 50/150: loss: 0.757526040077 60/150: loss: 0.0729400515556 70/150: loss: 0.0455974452198 80/150: loss: 0.17999586463 90/150: loss: 0.0215352103114 100/150: loss: 0.295116633177 110/150: loss: 0.0532150566578 120/150: loss: 0.0516105666757 130/150: loss: 0.149238437414 140/150: loss: 0.0896937176585 0/150: loss: 0.036860384047 10/150: loss: 0.449302881956 20/150: loss: 0.0413773059845 30/150: loss: 0.0302504710853 40/150: loss: 0.504677057266 50/150: loss: 0.0471465662122 60/150: loss: 0.0777884423733 70/150: loss: 0.306961596012 80/150: loss: 0.070302143693 90/150: loss: 0.0659918263555 100/150: loss: 0.0422312170267 110/150: loss: 0.0859310999513 120/150: loss: 0.144519984722 130/150: loss: 0.275940805674 140/150: loss: 0.0358592793345 0/150: loss: 0.105224251747 10/150: loss: 0.148995682597 20/150: loss: 0.171259567142 30/150: loss: 0.274571657181 40/150: loss: 0.172518834472 50/150: loss: 0.183918267488 60/150: loss: 0.0240714661777 70/150: loss: 0.0640301033854 80/150: loss: 0.0303267780691 90/150: loss: 0.0354539975524 100/150: loss: 0.321905910969 110/150: loss: 0.0354521796107 120/150: loss: 0.433114409447 130/150: loss: 0.274893432856 140/150: loss: 0.0201209746301 0/150: loss: 0.146205961704 10/150: loss: 0.024301096797 20/150: loss: 0.0766510739923 30/150: loss: 0.026312597096 40/150: loss: 0.216510772705 50/150: loss: 0.188184261322 60/150: loss: 0.0986335277557 70/150: loss: 0.0834529995918 80/150: loss: 0.0344540663064 90/150: loss: 0.036034502089 100/150: loss: 0.133203461766 110/150: loss: 0.245669692755 120/150: loss: 0.0325661376119 130/150: loss: 0.0658716857433 140/150: loss: 0.149314850569 0/150: loss: 0.160656109452 10/150: loss: 0.0239610373974 20/150: loss: 0.0743254423141 30/150: loss: 0.0337599217892 40/150: loss: 0.270701110363 50/150: loss: 0.0874256566167 60/150: loss: 0.191466525197 70/150: loss: 0.0451731085777 80/150: loss: 0.322745770216 90/150: loss: 0.35747885704 100/150: loss: 0.0230925567448 110/150: loss: 0.0326875969768 120/150: loss: 0.0688538253307 130/150: loss: 0.0896518230438 140/150: loss: 0.284733623266 0/150: loss: 0.0195708293468 10/150: loss: 0.153937295079 20/150: loss: 0.0591339208186 30/150: loss: 0.23615321517 40/150: loss: 0.44421517849 50/150: loss: 0.0530203543603 60/150: loss: 0.0956533029675 70/150: loss: 0.019025824964 80/150: loss: 0.0339080430567 90/150: loss: 0.101194664836 100/150: loss: 0.0948923230171 110/150: loss: 0.247991591692 120/150: loss: 0.0536427907646 130/150: loss: 0.114925198257 140/150: loss: 0.196949958801 0/150: loss: 0.153643086553 10/150: loss: 0.137717053294 20/150: loss: 0.165561944246 30/150: loss: 0.0275121647865 40/150: loss: 0.189765319228 50/150: loss: 0.259808897972 60/150: loss: 0.0327254422009 70/150: loss: 0.00838004332036 80/150: loss: 0.315408170223 90/150: loss: 0.405094146729 100/150: loss: 0.174818351865 110/150: loss: 0.171455651522 120/150: loss: 0.372536092997 130/150: loss: 0.123102478683 140/150: loss: 0.0146585721523 0/150: loss: 0.0336003229022 10/150: loss: 0.0829029455781 20/150: loss: 0.0116977188736 30/150: loss: 0.0294750221074 40/150: loss: 0.0531114600599 50/150: loss: 0.126261293888 60/150: loss: 0.0587904304266 70/150: loss: 0.0823902115226 80/150: loss: 0.381267011166 90/150: loss: 0.0595667436719 100/150: loss: 0.0555038079619 110/150: loss: 0.225528419018 120/150: loss: 0.013510087505 130/150: loss: 0.0658777728677 140/150: loss: 0.172976523638 0/150: loss: 0.132070481777 10/150: loss: 0.0125353559852 20/150: loss: 0.258884578943 30/150: loss: 0.333282023668 40/150: loss: 0.125652134418 50/150: loss: 0.0668507739902 60/150: loss: 0.448101401329 70/150: loss: 0.200519040227 80/150: loss: 0.0171897877008 90/150: loss: 0.220404684544 100/150: loss: 0.227313190699 110/150: loss: 0.0758602172136 120/150: loss: 0.089617036283 130/150: loss: 0.0501834079623 140/150: loss: 0.0200355537236 0/150: loss: 0.134692028165 10/150: loss: 0.170669481158 20/150: loss: 0.0556735247374 30/150: loss: 0.0251979921013 40/150: loss: 0.0370220169425 50/150: loss: 0.320994973183 60/150: loss: 0.0056255441159 70/150: loss: 0.185425505042 80/150: loss: 0.0976529866457 90/150: loss: 0.0857798457146 100/150: loss: 0.262543648481 110/150: loss: 0.0858322232962 120/150: loss: 0.0415281914175 130/150: loss: 0.194698363543../cleverhans/cleverhans/attacks.py:39: UserWarning: CleverHans support for supplying a callable instead of an instance of the cleverhans.model.Model class is deprecated and will be dropped on 2018-01-11.
  warnings.warn("CleverHans support for supplying a callable"
 140/150: loss: 0.205297455192 [*] Done with training 
 [-] Eval batch 0/58: acc: 0.545454545455 [-] Eval batch 1/58: acc: 0.538461538462 [-] Eval batch 2/58: acc: 0.546391752577 [-] Eval batch 3/58: acc: 0.581395348837 [-] Eval batch 4/58: acc: 0.596273291925 [-] Eval batch 5/58: acc: 0.611398963731 [-] Eval batch 6/58: acc: 0.626666666667 [-] Eval batch 7/58: acc: 0.610894941634 [-] Eval batch 8/58: acc: 0.636678200692 [-] Eval batch 9/58: acc: 0.647975077882 [-] Eval batch 10/58: acc: 0.651558073654 [-] Eval batch 11/58: acc: 0.65974025974 [-] Eval batch 12/58: acc: 0.671462829736 [-] Eval batch 13/58: acc: 0.672605790646 [-] Eval batch 14/58: acc: 0.673596673597 [-] Eval batch 15/58: acc: 0.676413255361 [-] Eval batch 16/58: acc: 0.678899082569 [-] Eval batch 17/58: acc: 0.677642980936 [-] Eval batch 18/58: acc: 0.674876847291 [-] Eval batch 19/58: acc: 0.677067082683 [-] Eval batch 20/58: acc: 0.673105497771 [-] Eval batch 21/58: acc: 0.669503546099 [-] Eval batch 22/58: acc: 0.675712347354 [-] Eval batch 23/58: acc: 0.67100130039 [-] Eval batch 24/58: acc: 0.664169787765 [-] Eval batch 25/58: acc: 0.662665066026 [-] Eval batch 26/58: acc: 0.663583815029 [-] Eval batch 27/58: acc: 0.663322185061 [-] Eval batch 28/58: acc: 0.663078579117 [-] Eval batch 29/58: acc: 0.660770031217 [-] Eval batch 30/58: acc: 0.668680765358 [-] Eval batch 31/58: acc: 0.670243902439 [-] Eval batch 32/58: acc: 0.668874172185 [-] Eval batch 33/58: acc: 0.673094582185 [-] Eval batch 34/58: acc: 0.671721677074 [-] Eval batch 35/58: acc: 0.672159583695 [-] Eval batch 36/58: acc: 0.674261603376 [-] Eval batch 37/58: acc: 0.675431388661 [-] Eval batch 38/58: acc: 0.678943154524 [-] Eval batch 39/58: acc: 0.679156908665 [-] Eval batch 40/58: acc: 0.675552170602 [-] Eval batch 41/58: acc: 0.675836431227 [-] Eval batch 42/58: acc: 0.674655047204 [-] Eval batch 43/58: acc: 0.677075940383 [-] Eval batch 44/58: acc: 0.678001387925 [-] Eval batch 45/58: acc: 0.67617107943 [-] Eval batch 46/58: acc: 0.675747508306 [-] Eval batch 47/58: acc: 0.675341574496 [-] Eval batch 48/58: acc: 0.674952198853 [-] Eval batch 49/58: acc: 0.675827607745 [-] Eval batch 50/58: acc: 0.675443968157 [-] Eval batch 51/58: acc: 0.676876876877 [-] Eval batch 52/58: acc: 0.681791396582 [-] Eval batch 53/58: acc: 0.683632157316 [-] Eval batch 54/58: acc: 0.683134582624 [-] Eval batch 55/58: acc: 0.68209704406 [-] Eval batch 56/58: acc: 0.681643835616 [-] Eval batch 57/58: acc: 0.680129240711 [*] Done with testing 
Test accuracy of oracle on adversarial examples generated using the substitute: 0.682702702703

Using TensorFlow backend.
2017-12-28 15:14:33.835173: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.835244: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.835252: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.835257: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:33.835262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:34.027481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 9.60GiB
2017-12-28 15:14:34.187686: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0xa4a21f0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-12-28 15:14:34.188479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:04:00.0
Total memory: 11.92GiB
Free memory: 11.38GiB
2017-12-28 15:14:34.188652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2017-12-28 15:14:34.188667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y 
2017-12-28 15:14:34.188672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y 
2017-12-28 15:14:34.188683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
2017-12-28 15:14:34.188689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)
../cleverhans/cleverhans/utils_tf.py:112: UserWarning: verbose argument is deprecated and will be removed on 2018-02-11. Instead, use utils.set_log_level(). For backward compatibility, log_level was set to logging.WARNING (30).
  warnings.warn("verbose argument is deprecated and will be removed"
(50000, 28, 28, 1)
[#] Max: 1.0, Min: 0.0
[*] Label : 3.0
[ 3.  9.  1. ...,  7.  5.  2.]
[ 3.  9.  1. ...,  7.  5.  2.]
0.89
<tensorflow.python.client.session.Session object at 0x9dabbd0>
 0/5: loss: 2.50958538055 0/5: loss: 2.11718082428 0/5: loss: 2.0536813736 0/5: loss: 1.82242000103 0/5: loss: 1.96285402775 0/5: loss: 1.431224823 0/5: loss: 1.43084084988 0/5: loss: 1.65015983582 0/5: loss: 1.4677863121 0/5: loss: 1.25308322906 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/10: loss: 2.42298364639 0/10: loss: 1.86912417412 0/10: loss: 1.64983510971 0/10: loss: 1.62590897083 0/10: loss: 1.56447994709 0/10: loss: 1.51441478729 0/10: loss: 1.55576252937 0/10: loss: 1.54441428185 0/10: loss: 1.44300198555 0/10: loss: 1.66959857941 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/19: loss: 1.23093771935 10/19: loss: 1.24861550331 0/19: loss: 1.50505256653 10/19: loss: 1.19426822662 0/19: loss: 0.951755642891 10/19: loss: 0.958021223545 0/19: loss: 1.3664804697 10/19: loss: 1.4643048048 0/19: loss: 1.03313326836 10/19: loss: 0.8374106884 0/19: loss: 1.114569664 10/19: loss: 1.22079324722 0/19: loss: 1.26159787178 10/19: loss: 1.24676191807 0/19: loss: 1.04091775417 10/19: loss: 0.877588629723 0/19: loss: 0.754230499268 10/19: loss: 1.36599588394 0/19: loss: 1.07879853249 10/19: loss: 0.725212335587 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/38: loss: 0.824204027653 10/38: loss: 0.735069870949 20/38: loss: 0.431368499994 30/38: loss: 0.615989744663 0/38: loss: 0.750410199165 10/38: loss: 0.618035376072 20/38: loss: 0.700674653053 30/38: loss: 0.80148178339 0/38: loss: 0.734092950821 10/38: loss: 0.787189781666 20/38: loss: 0.65903699398 30/38: loss: 0.723729014397 0/38: loss: 0.983855605125 10/38: loss: 0.906326949596 20/38: loss: 0.856155514717 30/38: loss: 0.966033041477 0/38: loss: 0.508900821209 10/38: loss: 0.572109282017 20/38: loss: 0.895892322063 30/38: loss: 0.692485332489 0/38: loss: 0.654032051563 10/38: loss: 0.404923468828 20/38: loss: 0.364660680294 30/38: loss: 0.85887491703 0/38: loss: 0.765027940273 10/38: loss: 0.443614393473 20/38: loss: 0.68771135807 30/38: loss: 0.714391112328 0/38: loss: 0.559360861778 10/38: loss: 0.499543637037 20/38: loss: 0.642302155495 30/38: loss: 0.698019206524 0/38: loss: 0.77063024044 10/38: loss: 0.540638804436 20/38: loss: 0.792441725731 30/38: loss: 0.444724470377 0/38: loss: 0.990594804287 10/38: loss: 0.879449784756 20/38: loss: 0.862038552761 30/38: loss: 0.650887131691 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/75: loss: 0.768303275108 10/75: loss: 0.731793284416 20/75: loss: 0.558896243572 30/75: loss: 0.419968783855 40/75: loss: 0.354387074709 50/75: loss: 0.478065162897 60/75: loss: 1.03358542919 70/75: loss: 0.393496543169 0/75: loss: 0.473492622375 10/75: loss: 0.520381748676 20/75: loss: 0.480285346508 30/75: loss: 0.280774533749 40/75: loss: 0.575099050999 50/75: loss: 0.439186751842 60/75: loss: 0.883103132248 70/75: loss: 0.652740001678 0/75: loss: 0.293435901403 10/75: loss: 0.317281931639 20/75: loss: 0.522032976151 30/75: loss: 0.348421007395 40/75: loss: 0.479977071285 50/75: loss: 0.638911485672 60/75: loss: 0.426131397486 70/75: loss: 0.502529263496 0/75: loss: 0.935435295105 10/75: loss: 0.79895055294 20/75: loss: 0.413170218468 30/75: loss: 0.346328496933 40/75: loss: 0.414502084255 50/75: loss: 0.574589490891 60/75: loss: 0.326115906239 70/75: loss: 0.676115214825 0/75: loss: 0.454732179642 10/75: loss: 0.448366343975 20/75: loss: 0.684054613113 30/75: loss: 0.200887769461 40/75: loss: 0.27009499073 50/75: loss: 0.208537474275 60/75: loss: 0.553395271301 70/75: loss: 0.419035762548 0/75: loss: 0.411877989769 10/75: loss: 0.239038676023 20/75: loss: 0.309537112713 30/75: loss: 0.522795379162 40/75: loss: 0.549146831036 50/75: loss: 0.450687557459 60/75: loss: 0.522471189499 70/75: loss: 0.289363890886 0/75: loss: 0.274852663279 10/75: loss: 0.448236227036 20/75: loss: 0.352072328329 30/75: loss: 0.254470229149 40/75: loss: 0.430683374405 50/75: loss: 0.285386621952 60/75: loss: 0.653534770012 70/75: loss: 0.410297691822 0/75: loss: 0.74403989315 10/75: loss: 0.302687257528 20/75: loss: 0.184679463506 30/75: loss: 0.458918631077 40/75: loss: 0.260382324457 50/75: loss: 0.371059238911 60/75: loss: 0.72417640686 70/75: loss: 0.442242532969 0/75: loss: 0.403084158897 10/75: loss: 0.884982526302 20/75: loss: 0.199248656631 30/75: loss: 0.482830226421 40/75: loss: 0.307660877705 50/75: loss: 0.411062598228 60/75: loss: 0.665306568146 70/75: loss: 0.270065128803 0/75: loss: 0.211070552468 10/75: loss: 0.346222907305 20/75: loss: 0.429048955441 30/75: loss: 0.520858585835 40/75: loss: 0.448560714722 50/75: loss: 0.431920230389 60/75: loss: 0.218934625387 70/75: loss: 0.325883030891 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/150: loss: 0.277643114328 10/150: loss: 0.151706531644 20/150: loss: 0.392493933439 30/150: loss: 0.311834543943 40/150: loss: 0.16180755198 50/150: loss: 0.479972720146 60/150: loss: 0.27562853694 70/150: loss: 0.608382284641 80/150: loss: 0.395832479 90/150: loss: 0.694839477539 100/150: loss: 0.422980070114 110/150: loss: 0.326204657555 120/150: loss: 0.161723434925 130/150: loss: 0.192907258868 140/150: loss: 0.195146858692 0/150: loss: 0.199237078428 10/150: loss: 0.594480991364 20/150: loss: 0.153461232781 30/150: loss: 0.0886806920171 40/150: loss: 0.53545987606 50/150: loss: 0.551649272442 60/150: loss: 0.104857742786 70/150: loss: 0.342126309872 80/150: loss: 0.168715119362 90/150: loss: 0.286862015724 100/150: loss: 0.500620424747 110/150: loss: 0.410960376263 120/150: loss: 0.301120400429 130/150: loss: 0.208032935858 140/150: loss: 0.0823492929339 0/150: loss: 0.266074717045 10/150: loss: 0.207413464785 20/150: loss: 0.235025003552 30/150: loss: 0.437204360962 40/150: loss: 0.111461535096 50/150: loss: 0.255902945995 60/150: loss: 0.166650786996 70/150: loss: 0.308931410313 80/150: loss: 0.147906243801 90/150: loss: 0.354044914246 100/150: loss: 0.338869810104 110/150: loss: 0.180092871189 120/150: loss: 0.319209605455 130/150: loss: 0.243231445551 140/150: loss: 0.279460966587 0/150: loss: 0.114969462156 10/150: loss: 0.293982863426 20/150: loss: 0.197709739208 30/150: loss: 0.126934468746 40/150: loss: 0.358287781477 50/150: loss: 0.229455888271 60/150: loss: 0.178783833981 70/150: loss: 0.150771826506 80/150: loss: 0.184285074472 90/150: loss: 0.305702000856 100/150: loss: 0.814250469208 110/150: loss: 0.349330306053 120/150: loss: 0.144261881709 130/150: loss: 0.189932256937 140/150: loss: 0.477157860994 0/150: loss: 0.156556323171 10/150: loss: 0.198997020721 20/150: loss: 0.256103515625 30/150: loss: 0.21013340354 40/150: loss: 0.359114170074 50/150: loss: 0.292310684919 60/150: loss: 0.157220885158 70/150: loss: 0.148046329618 80/150: loss: 0.18038085103 90/150: loss: 0.490265369415 100/150: loss: 0.138828128576 110/150: loss: 0.303301632404 120/150: loss: 0.267931878567 130/150: loss: 0.242458462715 140/150: loss: 0.244031473994 0/150: loss: 0.222878426313 10/150: loss: 0.173787266016 20/150: loss: 0.225117340684 30/150: loss: 0.605116546154 40/150: loss: 0.311439156532 50/150: loss: 0.23248834908 60/150: loss: 0.310744971037 70/150: loss: 0.173423677683 80/150: loss: 0.14528760314 90/150: loss: 0.130424529314 100/150: loss: 0.314975947142 110/150: loss: 0.207237392664 120/150: loss: 0.10800640285 130/150: loss: 0.365766108036 140/150: loss: 0.209996879101 0/150: loss: 0.443875133991 10/150: loss: 0.346961468458 20/150: loss: 0.126343220472 30/150: loss: 0.0908167362213 40/150: loss: 0.128140717745 50/150: loss: 0.32613503933 60/150: loss: 0.256513178349 70/150: loss: 0.229762107134 80/150: loss: 0.961755275726 90/150: loss: 0.451867818832 100/150: loss: 0.210653454065 110/150: loss: 0.266919255257 120/150: loss: 0.313051640987 130/150: loss: 0.27100032568 140/150: loss: 0.204559475183 0/150: loss: 0.171700626612 10/150: loss: 0.0978735387325 20/150: loss: 0.0901436507702 30/150: loss: 0.161236763 40/150: loss: 0.495715796947 50/150: loss: 0.186010435224 60/150: loss: 0.336550593376 70/150: loss: 0.323016762733 80/150: loss: 0.498844325542 90/150: loss: 0.180920496583 100/150: loss: 0.260347753763 110/150: loss: 0.487483859062 120/150: loss: 0.164464309812 130/150: loss: 0.107506141067 140/150: loss: 0.304988026619 0/150: loss: 0.373855441809 10/150: loss: 0.46697691083 20/150: loss: 0.281830489635 30/150: loss: 0.471505641937 40/150: loss: 0.138923451304 50/150: loss: 0.196655929089 60/150: loss: 0.341861337423 70/150: loss: 0.365163147449 80/150: loss: 0.0895658880472 90/150: loss: 0.121123015881 100/150: loss: 0.368293106556 110/150: loss: 0.215381622314 120/150: loss: 0.334387511015 130/150: loss: 0.184238255024 140/150: loss: 0.108924306929 0/150: loss: 0.574992895126 10/150: loss: 0.535498023033 20/150: loss: 0.238949030638 30/150: loss: 0.144217282534 40/150: loss: 0.135043829679 50/150: loss: 0.439756572247 60/150: loss: 0.132171809673 70/150: loss: 0.0839941054583 80/150: loss: 0.177681609988 90/150: loss: 0.192755803466 100/150: loss: 0.317241311073 110/150: loss: 0.359460473061 120/150: loss: 0.169018954039 130/150: loss: 0.461299628019../cleverhans/cleverhans/attacks.py:39: UserWarning: CleverHans support for supplying a callable instead of an instance of the cleverhans.model.Model class is deprecated and will be dropped on 2018-01-11.
  warnings.warn("CleverHans support for supplying a callable"
 140/150: loss: 0.309676498175 [*] Done with training 
 [-] Eval batch 0/58: acc: 0.818181818182 [-] Eval batch 1/58: acc: 0.753846153846 [-] Eval batch 2/58: acc: 0.721649484536 [-] Eval batch 3/58: acc: 0.682170542636 [-] Eval batch 4/58: acc: 0.708074534161 [-] Eval batch 5/58: acc: 0.715025906736 [-] Eval batch 6/58: acc: 0.715555555556 [-] Eval batch 7/58: acc: 0.731517509728 [-] Eval batch 8/58: acc: 0.726643598616 [-] Eval batch 9/58: acc: 0.728971962617 [-] Eval batch 10/58: acc: 0.725212464589 [-] Eval batch 11/58: acc: 0.732467532468 [-] Eval batch 12/58: acc: 0.729016786571 [-] Eval batch 13/58: acc: 0.728285077951 [-] Eval batch 14/58: acc: 0.733887733888 [-] Eval batch 15/58: acc: 0.732943469786 [-] Eval batch 16/58: acc: 0.730275229358 [-] Eval batch 17/58: acc: 0.726169844021 [-] Eval batch 18/58: acc: 0.72249589491 [-] Eval batch 19/58: acc: 0.728549141966 [-] Eval batch 20/58: acc: 0.731054977712 [-] Eval batch 21/58: acc: 0.73475177305 [-] Eval batch 22/58: acc: 0.735413839891 [-] Eval batch 23/58: acc: 0.732119635891 [-] Eval batch 24/58: acc: 0.731585518102 [-] Eval batch 25/58: acc: 0.734693877551 [-] Eval batch 26/58: acc: 0.732947976879 [-] Eval batch 27/58: acc: 0.733556298774 [-] Eval batch 28/58: acc: 0.734122712594 [-] Eval batch 29/58: acc: 0.728407908429 [-] Eval batch 30/58: acc: 0.725075528701 [-] Eval batch 31/58: acc: 0.723902439024 [-] Eval batch 32/58: acc: 0.719962157048 [-] Eval batch 33/58: acc: 0.719926538108 [-] Eval batch 34/58: acc: 0.722569134701 [-] Eval batch 35/58: acc: 0.724197745013 [-] Eval batch 36/58: acc: 0.723206751055 [-] Eval batch 37/58: acc: 0.727198027938 [-] Eval batch 38/58: acc: 0.726981585268 [-] Eval batch 39/58: acc: 0.729898516784 [-] Eval batch 40/58: acc: 0.731150038081 [-] Eval batch 41/58: acc: 0.732342007435 [-] Eval batch 42/58: acc: 0.732026143791 [-] Eval batch 43/58: acc: 0.732434350603 [-] Eval batch 44/58: acc: 0.733518390007 [-] Eval batch 45/58: acc: 0.733197556008 [-] Eval batch 46/58: acc: 0.732890365449 [-] Eval batch 47/58: acc: 0.733246584255 [-] Eval batch 48/58: acc: 0.734862970045 [-] Eval batch 49/58: acc: 0.732667083073 [-] Eval batch 50/58: acc: 0.733006736069 [-] Eval batch 51/58: acc: 0.732132132132 [-] Eval batch 52/58: acc: 0.734236888627 [-] Eval batch 53/58: acc: 0.734528629265 [-] Eval batch 54/58: acc: 0.734809767178 [-] Eval batch 55/58: acc: 0.730619074177 [-] Eval batch 56/58: acc: 0.731506849315 [-] Eval batch 57/58: acc: 0.72859450727 [*] Done with testing 
Test accuracy of oracle on adversarial examples generated using the substitute: 0.731351351351

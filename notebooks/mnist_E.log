Using TensorFlow backend.
2017-12-28 15:14:43.980892: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:43.980938: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:43.980945: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:43.980950: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:43.980955: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-28 15:14:44.147461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:03:00.0
Total memory: 11.92GiB
Free memory: 9.15GiB
2017-12-28 15:14:44.298070: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x9cd9260 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2017-12-28 15:14:44.298841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:04:00.0
Total memory: 11.92GiB
Free memory: 11.17GiB
2017-12-28 15:14:44.299008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 
2017-12-28 15:14:44.299017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y 
2017-12-28 15:14:44.299022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y 
2017-12-28 15:14:44.299031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)
2017-12-28 15:14:44.299037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:04:00.0)
../cleverhans/cleverhans/utils_tf.py:112: UserWarning: verbose argument is deprecated and will be removed on 2018-02-11. Instead, use utils.set_log_level(). For backward compatibility, log_level was set to logging.WARNING (30).
  warnings.warn("verbose argument is deprecated and will be removed"
(50000, 28, 28, 1)
[#] Max: 1.0, Min: 0.0
[*] Label : 9.0
[ 9.  6.  9. ...,  7.  5.  6.]
[ 9.  6.  9. ...,  7.  5.  6.]
0.974
<tensorflow.python.client.session.Session object at 0x9620b50>
 0/5: loss: 2.34079837799 0/5: loss: 2.01412630081 0/5: loss: 1.47002506256 0/5: loss: 1.15587234497 0/5: loss: 0.903188347816 0/5: loss: 0.544835090637 0/5: loss: 0.358841300011 0/5: loss: 0.186950802803 0/5: loss: 0.188324570656 0/5: loss: 0.130865961313 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/10: loss: 1.75870501995 0/10: loss: 1.02144265175 0/10: loss: 1.12072515488 0/10: loss: 1.08289194107 0/10: loss: 0.846993863583 0/10: loss: 0.654361248016 0/10: loss: 0.785646915436 0/10: loss: 0.66003215313 0/10: loss: 0.586148202419 0/10: loss: 0.623753547668 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/19: loss: 0.865955531597 10/19: loss: 0.707651853561 0/19: loss: 0.733189940453 10/19: loss: 0.750282585621 0/19: loss: 0.495888829231 10/19: loss: 0.651628255844 0/19: loss: 0.704337477684 10/19: loss: 0.538649916649 0/19: loss: 0.398721456528 10/19: loss: 0.255003333092 0/19: loss: 0.28191947937 10/19: loss: 0.530644953251 0/19: loss: 0.320221692324 10/19: loss: 0.52466994524 0/19: loss: 0.217493712902 10/19: loss: 0.276426762342 0/19: loss: 0.204975008965 10/19: loss: 0.54559648037 0/19: loss: 0.266376256943 10/19: loss: 0.0923568159342 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/38: loss: 0.293909072876 10/38: loss: 0.535140991211 20/38: loss: 0.156997486949 30/38: loss: 0.197372078896 0/38: loss: 0.246345639229 10/38: loss: 0.290305942297 20/38: loss: 0.581235945225 30/38: loss: 0.271494358778 0/38: loss: 0.305233120918 10/38: loss: 0.461445987225 20/38: loss: 0.177406221628 30/38: loss: 0.270931661129 0/38: loss: 0.310630977154 10/38: loss: 0.148880526423 20/38: loss: 0.500389575958 30/38: loss: 0.325041770935 0/38: loss: 0.274243056774 10/38: loss: 0.240768909454 20/38: loss: 0.304024934769 30/38: loss: 0.329273283482 0/38: loss: 0.151291742921 10/38: loss: 0.195555835962 20/38: loss: 0.192077234387 30/38: loss: 0.441031932831 0/38: loss: 0.381675094366 10/38: loss: 0.210672795773 20/38: loss: 0.226447492838 30/38: loss: 0.319555521011 0/38: loss: 0.297971397638 10/38: loss: 0.225871309638 20/38: loss: 0.352158099413 30/38: loss: 0.259101897478 0/38: loss: 0.3572948277 10/38: loss: 0.179418683052 20/38: loss: 0.179331317544 30/38: loss: 0.0664734244347 0/38: loss: 0.264944046736 10/38: loss: 0.238300412893 20/38: loss: 0.202796876431 30/38: loss: 0.23310662806 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/75: loss: 0.681489527225 10/75: loss: 0.318699449301 20/75: loss: 0.102201700211 30/75: loss: 0.0498001277447 40/75: loss: 0.0470517650247 50/75: loss: 0.205807417631 60/75: loss: 0.439437896013 70/75: loss: 0.172144114971 0/75: loss: 0.0411987230182 10/75: loss: 0.170435577631 20/75: loss: 0.326326370239 30/75: loss: 0.0258653908968 40/75: loss: 0.240967810154 50/75: loss: 0.175740376115 60/75: loss: 0.288297832012 70/75: loss: 0.507694840431 0/75: loss: 0.164902076125 10/75: loss: 0.0595907792449 20/75: loss: 0.265038281679 30/75: loss: 0.146269738674 40/75: loss: 0.0364770144224 50/75: loss: 0.258997410536 60/75: loss: 0.0319497548044 70/75: loss: 0.0541927106678 0/75: loss: 0.170740291476 10/75: loss: 0.165556207299 20/75: loss: 0.116097152233 30/75: loss: 0.0589174628258 40/75: loss: 0.119083620608 50/75: loss: 0.110161013901 60/75: loss: 0.028946718201 70/75: loss: 0.264174163342 0/75: loss: 0.0398870706558 10/75: loss: 0.172878652811 20/75: loss: 0.367953240871 30/75: loss: 0.162754148245 40/75: loss: 0.0269630625844 50/75: loss: 0.0318901911378 60/75: loss: 0.0147858159617 70/75: loss: 0.129436522722 0/75: loss: 0.0968448221684 10/75: loss: 0.050501909107 20/75: loss: 0.0797096788883 30/75: loss: 0.260042905807 40/75: loss: 0.239347681403 50/75: loss: 0.304860383272 60/75: loss: 0.0145964529365 70/75: loss: 0.336339116096 0/75: loss: 0.0104897804558 10/75: loss: 0.156464055181 20/75: loss: 0.0513511188328 30/75: loss: 0.22412276268 40/75: loss: 0.156923890114 50/75: loss: 0.1523796767 60/75: loss: 0.104170635343 70/75: loss: 0.189094781876 0/75: loss: 0.308354139328 10/75: loss: 0.0599519349635 20/75: loss: 0.211231425405 30/75: loss: 0.0276599060744 40/75: loss: 0.183707907796 50/75: loss: 0.109127894044 60/75: loss: 0.571471333504 70/75: loss: 0.266827613115 0/75: loss: 0.168363243341 10/75: loss: 0.356307506561 20/75: loss: 0.0493420101702 30/75: loss: 0.021682202816 40/75: loss: 0.0287280753255 50/75: loss: 0.0248521342874 60/75: loss: 0.234934449196 70/75: loss: 0.127772152424 0/75: loss: 0.0954706668854 10/75: loss: 0.0884657502174 20/75: loss: 0.136583983898 30/75: loss: 0.189317807555 40/75: loss: 0.142548218369 50/75: loss: 0.174696609378 60/75: loss: 0.017049016431 70/75: loss: 0.278174877167 [*] Done with training 
Augmenting substitute training data.
Labeling substitute training data.
 0/150: loss: 0.00270849093795 10/150: loss: 0.00210628542118 20/150: loss: 0.168271407485 30/150: loss: 0.0088646793738 40/150: loss: 0.100659035146 50/150: loss: 0.575168251991 60/150: loss: 0.0208989158273 70/150: loss: 0.0101769454777 80/150: loss: 0.335787802935 90/150: loss: 0.0241776760668 100/150: loss: 0.393177449703 110/150: loss: 0.0184045657516 120/150: loss: 0.00562850804999 130/150: loss: 0.0361320488155 140/150: loss: 0.0798203796148 0/150: loss: 0.0201878789812 10/150: loss: 0.454474359751 20/150: loss: 0.0129736103117 30/150: loss: 0.0097216758877 40/150: loss: 0.439828187227 50/150: loss: 0.0158178433776 60/150: loss: 0.0217529535294 70/150: loss: 0.0821650326252 80/150: loss: 0.0117418244481 90/150: loss: 0.00847258791327 100/150: loss: 0.0167042054236 110/150: loss: 0.0203620549291 120/150: loss: 0.0204558372498 130/150: loss: 0.0481976494193 140/150: loss: 0.00497356057167 0/150: loss: 0.0175513457507 10/150: loss: 0.0146535383537 20/150: loss: 0.0141473533586 30/150: loss: 0.109309569001 40/150: loss: 0.0101693607867 50/150: loss: 0.10547272861 60/150: loss: 0.010950313881 70/150: loss: 0.0311696305871 80/150: loss: 0.0123172365129 90/150: loss: 0.0110296886414 100/150: loss: 0.225709632039 110/150: loss: 0.0087523041293 120/150: loss: 0.251933038235 130/150: loss: 0.238046854734 140/150: loss: 0.00926782935858 0/150: loss: 0.00868857838213 10/150: loss: 0.0172218158841 20/150: loss: 0.0156215848401 30/150: loss: 0.016356036067 40/150: loss: 0.315739750862 50/150: loss: 0.176204711199 60/150: loss: 0.0187643989921 70/150: loss: 0.0128268040717 80/150: loss: 0.0758084058762 90/150: loss: 0.013483652845 100/150: loss: 0.0851398557425 110/150: loss: 0.152919262648 120/150: loss: 0.155982613564 130/150: loss: 0.0528612434864 140/150: loss: 0.0285239126533 0/150: loss: 0.0239022597671 10/150: loss: 0.00891453027725 20/150: loss: 0.0149726346135 30/150: loss: 0.00334952073172 40/150: loss: 0.0186111405492 50/150: loss: 0.00885807164013 60/150: loss: 0.0102616921067 70/150: loss: 0.0516709387302 80/150: loss: 0.0767382308841 90/150: loss: 0.142620056868 100/150: loss: 0.0169809274375 110/150: loss: 0.0178014338017 120/150: loss: 0.0192857161164 130/150: loss: 0.124620199203 140/150: loss: 0.0701416656375 0/150: loss: 0.0274879559875 10/150: loss: 0.00482171960175 20/150: loss: 0.00949417799711 30/150: loss: 0.146087154746 40/150: loss: 0.236825346947 50/150: loss: 0.0201527103782 60/150: loss: 0.00762211531401 70/150: loss: 0.0102840578184 80/150: loss: 0.17316031456 90/150: loss: 0.0306762717664 100/150: loss: 0.03295635432 110/150: loss: 0.142382442951 120/150: loss: 0.0286084655672 130/150: loss: 0.117846556008 140/150: loss: 0.00314253591932 0/150: loss: 0.212577342987 10/150: loss: 0.170107245445 20/150: loss: 0.0112663591281 30/150: loss: 0.00489948969334 40/150: loss: 0.177400678396 50/150: loss: 0.0688257664442 60/150: loss: 0.0107248220593 70/150: loss: 0.00691026914865 80/150: loss: 0.21456630528 90/150: loss: 0.145410522819 100/150: loss: 0.0740508213639 110/150: loss: 0.0120814926922 120/150: loss: 0.209174513817 130/150: loss: 0.0566404014826 140/150: loss: 0.00845877546817 0/150: loss: 0.0153887216002 10/150: loss: 0.0286455377936 20/150: loss: 0.00628622807562 30/150: loss: 0.0105370394886 40/150: loss: 0.0106819923967 50/150: loss: 0.0110757024959 60/150: loss: 0.136447995901 70/150: loss: 0.00373402377591 80/150: loss: 0.0729376450181 90/150: loss: 0.0113656483591 100/150: loss: 0.0143277551979 110/150: loss: 0.0145425554365 120/150: loss: 0.00761763146147 130/150: loss: 0.0102740461007 140/150: loss: 0.139327988029 0/150: loss: 0.0409460403025 10/150: loss: 0.00946926511824 20/150: loss: 0.104357920587 30/150: loss: 0.258187800646 40/150: loss: 0.0192858818918 50/150: loss: 0.062850818038 60/150: loss: 0.230429530144 70/150: loss: 0.0105781415477 80/150: loss: 0.00363877485506 90/150: loss: 0.0109241809696 100/150: loss: 0.15068538487 110/150: loss: 0.028394125402 120/150: loss: 0.0423268638551 130/150: loss: 0.017747534439 140/150: loss: 0.0506109595299 0/150: loss: 0.0766116529703 10/150: loss: 0.143763154745 20/150: loss: 0.0267260819674 30/150: loss: 0.0131185539067 40/150: loss: 0.00492736371234 50/150: loss: 0.120461255312 60/150: loss: 0.00107527594082 70/150: loss: 0.00255865626968 80/150: loss: 0.0152579005808 90/150: loss: 0.0257258247584 100/150: loss: 0.00916690099984 110/150: loss: 0.0155688971281 120/150: loss: 0.00908630900085 130/150: loss: 0.190202727914../cleverhans/cleverhans/attacks.py:39: UserWarning: CleverHans support for supplying a callable instead of an instance of the cleverhans.model.Model class is deprecated and will be dropped on 2018-01-11.
  warnings.warn("CleverHans support for supplying a callable"
 140/150: loss: 0.165003329515 [*] Done with training 
 [-] Eval batch 0/58: acc: 0.848484848485 [-] Eval batch 1/58: acc: 0.876923076923 [-] Eval batch 2/58: acc: 0.886597938144 [-] Eval batch 3/58: acc: 0.914728682171 [-] Eval batch 4/58: acc: 0.925465838509 [-] Eval batch 5/58: acc: 0.922279792746 [-] Eval batch 6/58: acc: 0.928888888889 [-] Eval batch 7/58: acc: 0.922178988327 [-] Eval batch 8/58: acc: 0.930795847751 [-] Eval batch 9/58: acc: 0.931464174455 [-] Eval batch 10/58: acc: 0.932011331445 [-] Eval batch 11/58: acc: 0.92987012987 [-] Eval batch 12/58: acc: 0.928057553957 [-] Eval batch 13/58: acc: 0.928730512249 [-] Eval batch 14/58: acc: 0.925155925156 [-] Eval batch 15/58: acc: 0.927875243665 [-] Eval batch 16/58: acc: 0.924770642202 [-] Eval batch 17/58: acc: 0.927209705373 [-] Eval batch 18/58: acc: 0.91789819376 [-] Eval batch 19/58: acc: 0.91887675507 [-] Eval batch 20/58: acc: 0.916790490342 [-] Eval batch 21/58: acc: 0.91914893617 [-] Eval batch 22/58: acc: 0.919945725916 [-] Eval batch 23/58: acc: 0.921976592978 [-] Eval batch 24/58: acc: 0.923845193508 [-] Eval batch 25/58: acc: 0.925570228091 [-] Eval batch 26/58: acc: 0.926011560694 [-] Eval batch 27/58: acc: 0.926421404682 [-] Eval batch 28/58: acc: 0.927879440258 [-] Eval batch 29/58: acc: 0.926118626431 [-] Eval batch 30/58: acc: 0.92749244713 [-] Eval batch 31/58: acc: 0.929756097561 [-] Eval batch 32/58: acc: 0.929990539262 [-] Eval batch 33/58: acc: 0.931129476584 [-] Eval batch 34/58: acc: 0.932203389831 [-] Eval batch 35/58: acc: 0.932350390286 [-] Eval batch 36/58: acc: 0.933333333333 [-] Eval batch 37/58: acc: 0.935086277732 [-] Eval batch 38/58: acc: 0.934347477982 [-] Eval batch 39/58: acc: 0.935206869633 [-] Eval batch 40/58: acc: 0.936024371668 [-] Eval batch 41/58: acc: 0.937546468401 [-] Eval batch 42/58: acc: 0.937545388526 [-] Eval batch 43/58: acc: 0.9375443577 [-] Eval batch 44/58: acc: 0.937543372658 [-] Eval batch 45/58: acc: 0.937542430414 [-] Eval batch 46/58: acc: 0.936877076412 [-] Eval batch 47/58: acc: 0.936890045543 [-] Eval batch 48/58: acc: 0.93690248566 [-] Eval batch 49/58: acc: 0.936289818863 [-] Eval batch 50/58: acc: 0.937538273117 [-] Eval batch 51/58: acc: 0.938138138138 [-] Eval batch 52/58: acc: 0.937536829699 [-] Eval batch 53/58: acc: 0.938692886061 [-] Eval batch 54/58: acc: 0.939806927882 [-] Eval batch 55/58: acc: 0.940323480201 [-] Eval batch 56/58: acc: 0.939726027397 [-] Eval batch 57/58: acc: 0.937533656435 [*] Done with testing 
Test accuracy of oracle on adversarial examples generated using the substitute: 0.941081081081
